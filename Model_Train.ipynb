{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20eac1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tweet_id                                              tweet  \\\n",
      "0              tweet_id                                              tweet   \n",
      "1    837746741063593992  A good over by hasan ali but iska over bachana...   \n",
      "2    838703117810085888  Dogs are loyal, par un he se koi shadi nahi ka...   \n",
      "3    763674497933336576   Apny bachon ko sikhaon ga \"Grades don't matter.\"   \n",
      "4    748246556998959104         Highlight of the night\\nKe lights nai hai.   \n",
      "..                  ...                                                ...   \n",
      "739  838434401130278913        Current run rate dekh kar sharam a Rai hay!   \n",
      "740  838456430848061445                Pakistan ki khushi over everything    \n",
      "741  838717126277898240  One more day where I've decided, KAL SE PAKKA ...   \n",
      "742  842114263921614850  Sony se pehly apny favt songs ki lyrics post k...   \n",
      "743  842405293896134656  Mardam shumari mai double faced logo ka kya ho...   \n",
      "\n",
      "                                                tokens  \\\n",
      "0                                            ['tweet']   \n",
      "1    ['A', 'good', 'over', 'by', 'hasan', 'ali', 'b...   \n",
      "2    ['Dogs', 'are', 'loyal', ',', 'par', 'un', 'he...   \n",
      "3    ['Apny', 'bachon', 'ko', 'sikhaon', 'ga', '\"',...   \n",
      "4    ['Highlight', 'of', 'the', 'night', 'Ke', 'lig...   \n",
      "..                                                 ...   \n",
      "739  ['Current', 'run', 'rate', 'dekh', 'kar', 'sha...   \n",
      "740  ['Pakistan', 'ki', 'khushi', 'over', 'everythi...   \n",
      "741  ['One', 'more', 'day', 'where', 'I', \"'ve\", 'd...   \n",
      "742  ['Sony', 'se', 'pehly', 'apny', 'favt', 'songs...   \n",
      "743  ['Mardam', 'shumari', 'mai', 'double', 'faced'...   \n",
      "\n",
      "                                        semantic_roles  \\\n",
      "0                                                   []   \n",
      "1    ['_', '_', 'ARG1', '_', '_', 'ARG3', '_', '_',...   \n",
      "2    ['ARG1', '_', 'ARG2_ATTR', '_', '_', 'ARGM_MNR...   \n",
      "3    ['_', '_', 'ARG2', '_', '_', '_', 'ARG1', '_',...   \n",
      "4    ['_', '_', '_', '_', '_', '_', 'ARG1', 'ARGM_N...   \n",
      "..                                                 ...   \n",
      "739  ['_', '_', 'ARG1', '_', '_', 'ARGM_PRX', '_', ...   \n",
      "740                ['_', '_', '_', '_', '_', '_', '_']   \n",
      "741  ['_', '_', 'ARGM_TMP', 'ARGM_MNR', 'ARG0', '_'...   \n",
      "742  ['ARGM_TMP', '_', '_', '_', '_', '_', '_', 'AR...   \n",
      "743  ['_', 'ARGM_LOC', '_', '_', '_', 'ARG1', '_', ...   \n",
      "\n",
      "                                          language_ids  \\\n",
      "0                                                   []   \n",
      "1    ['en', 'en', 'en', 'en', 'ne', 'ne', 'NULL', '...   \n",
      "2    ['en', 'en', 'en', 'univ', 'hi', 'hi', 'hi', '...   \n",
      "3    ['univ', 'hi', 'hi', 'hi', 'hi', 'univ', 'en',...   \n",
      "4    ['univ', 'en', 'en', 'en', 'en', 'hi', 'en', '...   \n",
      "..                                                 ...   \n",
      "739  ['en', 'en', 'en', 'hi', 'hi', 'hi', 'hi', 'ne...   \n",
      "740     ['ne', 'hi', 'hi', 'NULL', 'en', 'en', 'univ']   \n",
      "741  ['en', 'en', 'en', 'en', 'en', 'en', 'en', 'un...   \n",
      "742  ['hi', 'hi', 'hi', 'hi', 'en', 'en', 'hi', 'en...   \n",
      "743  ['ne', 'ne', 'hi', 'en', 'en', 'hi', 'hi', 'hi...   \n",
      "\n",
      "                                              pos_tags  \n",
      "0                                                   []  \n",
      "1    ['DET', 'JJ', 'NN', 'PSP', 'NNPC', 'NNP', 'VM'...  \n",
      "2    ['NN', 'VM', 'JJ', 'PUNC', 'CC', 'PRP', 'PSP',...  \n",
      "3    ['PUNC', 'PRP', 'NN', 'PSP', 'VM', 'PUNC', 'NN...  \n",
      "4    ['PUNC', 'NN', 'PSP', 'DET', 'NN', 'CC', 'NN',...  \n",
      "..                                                 ...  \n",
      "739  ['JJ', 'NNC', 'NN', 'VM', 'VAUX', 'NN', 'VM', ...  \n",
      "740  ['NNP', 'PSP', 'NN', 'CC_CCD', 'PSP', 'NN', 'S...  \n",
      "741  ['QC', 'JJ', 'NN', 'RB', 'PRP', 'VAUX', 'VM', ...  \n",
      "742  ['VM', 'PSP', 'NST', 'PRP', 'JJ', 'NN', 'PSP',...  \n",
      "743  ['NNPC', 'NNP', 'PSP', 'JJC', 'JJ', 'NN', 'PSP...  \n",
      "\n",
      "[744 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\bpsad\\Desktop\\AIML_SRL\\AIML _NLP_PROJECT\\output_sem_role_zr1.csv\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f4470d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id          0\n",
      "tweet             0\n",
      "tokens            0\n",
      "semantic_roles    0\n",
      "language_ids      0\n",
      "pos_tags          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df = df[['tweet_id', 'semantic_roles', 'tweet','tokens','language_ids','pos_tags' ]]\n",
    "# Assuming 'Semantic_Role' is a string representation of a list, for example: \"['ARG1', '_', 'ARG2']\"\n",
    "df['semantic_roles'] = df['semantic_roles'].apply(eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e6174ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "validation_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df2b3b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the distribution of token lengths in your dataset\n",
    "token_lengths = [len(tokenizer.encode(text)) for text in df['tweet']]\n",
    "plt.hist(token_lengths, bins=30)\n",
    "plt.title('Distribution of Token Lengths')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Choose MAX_LENGTH based on the visualization and your computational resources\n",
    "MAX_LENGTH = 128  # Adjust this value based on your analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9afaaf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('l3cube-pune/hing-mbert')\n",
    "PAD_TOKEN_ID = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=25,  # Define your maximum sequence length\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "train_tokens = [tokenize_text(text) for text in train_df['tweet']]\n",
    "validation_tokens = [tokenize_text(text) for text in validation_df['tweet']]\n",
    "test_tokens = [tokenize_text(text) for text in test_df['tweet']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8714bdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of self.semantic_roles: 595\n",
      "Range of indices: range(0, 595)\n",
      "Length of self.semantic_roles: 74\n",
      "Range of indices: range(0, 74)\n",
      "Length of self.semantic_roles: 75\n",
      "Range of indices: range(0, 75)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SRLDataset(Dataset):\n",
    "    def __init__(self, tokens, semantic_roles):\n",
    "        self.tokens = tokens\n",
    "        self.semantic_roles = semantic_roles\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        print(\"Length of self.semantic_roles:\", len(self.semantic_roles))\n",
    "        print(\"Range of indices:\", range(len(self.semantic_roles)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokens[idx]['input_ids']\n",
    "        attention_mask = self.tokens[idx]['attention_mask']\n",
    "\n",
    "        # Check if idx is within the valid range of indices\n",
    "        if 0 <= idx < len(self.semantic_roles):\n",
    "            semantic_roles = self.semantic_roles[idx]\n",
    "            # Tokenize input features\n",
    "            # Convert labels to numerical format using label encoding\n",
    "            encoded_roles = self.label_encoder.fit_transform(semantic_roles)\n",
    "\n",
    "            # Determine the maximum sequence length in the batch\n",
    "            max_len = max(len(ids) for ids in input_ids)\n",
    "\n",
    "            # Pad 'input_ids' and 'attention_mask' for the entire batch\n",
    "            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "            attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "            # Pad or truncate 'input_ids' and 'attention_mask' to have the same length\n",
    "            input_ids = input_ids[:, :max_len]\n",
    "            attention_mask = attention_mask[:, :max_len]\n",
    "\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'semantic_roles': torch.tensor(encoded_roles, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            # Handle the case where idx is out of range\n",
    "            raise IndexError(f\"Index {idx} is out of range for semantic_roles list.\")\n",
    "\n",
    "            \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['semantic_roles'] for item in batch]\n",
    "\n",
    "    # Find the maximum length in the batch\n",
    "    max_len = max(len(seq[0]) for seq in sequences)\n",
    "\n",
    "    # Ensure the batch size remains consistent\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    # Pad sequences and labels to the maximum length\n",
    "    padded_sequences = rnn_utils.pad_sequence([seq[0] for seq in sequences], batch_first=True, padding_value=PAD_TOKEN_ID)\n",
    "    padded_attention_masks = rnn_utils.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Truncate or pad labels to the maximum length\n",
    "    padded_labels = [F.pad(label, (0, max_len - len(label)), value=-100) for label in labels]\n",
    "\n",
    "    # Stack labels\n",
    "    stacked_labels = torch.stack(padded_labels, dim=0)\n",
    "\n",
    "    return {\n",
    "        'input_ids': padded_sequences,\n",
    "        'attention_mask': padded_attention_masks,\n",
    "        'semantic_roles': stacked_labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Create datasets and data loaders with collate_fn\n",
    "train_dataset = SRLDataset(train_tokens, train_df['semantic_roles'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "validation_dataset = SRLDataset(validation_tokens, validation_df['semantic_roles'])\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = SRLDataset(test_tokens, test_df['semantic_roles'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a0ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your 'Semantic_Role' column is already converted to lists\n",
    "unique_labels = set(label for labels in df['semantic_roles'] for label in labels)\n",
    "num_labels = len(unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f2c820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda08503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ARGM_MNR', 'ARGM_DIS', 'ARG1', 'ARG0', 'ARGM_NEG', 'ARG2_ATTr', 'ARG2_ATTRas', 'ARG2_SO', 'ARG2_SOU', 'ARGM_CAU', '_', 'ARGM_DIR', 'ARG3', 'ARG2_GOL', 'ARG2_LOC', 'ARG2', 'ARGM_EXT', 'ARGM_ADV', 'ARG2_ATTR', 'ARG2_CAU', 'ARG2_MNR', 'ARGM_PRP', 'ARGM_TMP', 'ARGM_PRX', 'ARGM_LOC'}\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'Semantic_Role' column is already converted to lists\n",
    "unique_labels = set(label for labels in df['semantic_roles'] for label in labels)\n",
    "print(unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4023beef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Labels: ['ARGM_DIR', 'ARGM_ADV', 'ARG2_ATTR', 'ARG3', 'ARGM_CAU', 'ARG1', 'ARG2_SOU', 'ARGM_TMP', 'ARG2_LOC', 'ARG0', 'ARGM_MNR', 'ARGM_NEG', 'ARG2_CAU', 'ARG2', 'ARG2_MNR', 'ARGM_PRP', 'ARGM_LOC', 'ARG2_GOL', 'ARG2_ATTr', 'ARGM_DIS', 'ARGM_EXT', '_', 'ARGM_PRX', 'ARG2_ATTRas']\n",
      "Encoded Labels: [14 12  3 11 13  1 10 22  8  0 18 19  6  2  9 20 17  7  5 15 16 23 21  4]\n",
      "Decoded Labels: ['ARGM_DIR' 'ARGM_ADV' 'ARG2_ATTR' 'ARG3' 'ARGM_CAU' 'ARG1' 'ARG2_SOU'\n",
      " 'ARGM_TMP' 'ARG2_LOC' 'ARG0' 'ARGM_MNR' 'ARGM_NEG' 'ARG2_CAU' 'ARG2'\n",
      " 'ARG2_MNR' 'ARGM_PRP' 'ARGM_LOC' 'ARG2_GOL' 'ARG2_ATTr' 'ARGM_DIS'\n",
      " 'ARGM_EXT' '_' 'ARGM_PRX' 'ARG2_ATTRas']\n"
     ]
    }
   ],
   "source": [
    "labels = list(unique_labels)\n",
    "label_to_index = {label: i for i, label in enumerate(labels)}\n",
    "index_to_label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "num_labels = len(labels)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example labels\n",
    "semantic_roles = ['ARGM_DIR', 'ARGM_ADV', 'ARG2_ATTR', 'ARG3', 'ARGM_CAU', 'ARG1', 'ARG2_SOU', 'ARGM_TMP', 'ARG2_LOC', 'ARG0', 'ARGM_MNR', 'ARGM_NEG', 'ARG2_CAU', 'ARG2', 'ARG2_MNR', 'ARGM_PRP', 'ARGM_LOC', 'ARG2_GOL', 'ARG2_ATTr', 'ARGM_DIS', 'ARGM_EXT', '_', 'ARGM_PRX', 'ARG2_ATTRas']\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels\n",
    "encoded_roles = label_encoder.fit_transform(semantic_roles)\n",
    "\n",
    "print(\"Original Labels:\", semantic_roles)\n",
    "print(\"Encoded Labels:\", encoded_roles)\n",
    "\n",
    "# To inverse transform back to original labels\n",
    "decoded_roles = label_encoder.inverse_transform(encoded_roles)\n",
    "print(\"Decoded Labels:\", decoded_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b278e607",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Anaconda\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at l3cube-pune/hing-mbert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Anaconda\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Epoch 1, Validation Accuracy: 0.8171428571428572\n",
      "Epoch 2, Validation Accuracy: 0.8430252100840336\n",
      "Epoch 3, Validation Accuracy: 0.8766386554621849\n",
      "Epoch 4, Validation Accuracy: 0.8847058823529412\n",
      "Epoch 5, Validation Accuracy: 0.879327731092437\n",
      "Fold 2/5\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Epoch 1, Validation Accuracy: 0.8715966386554622\n",
      "Epoch 2, Validation Accuracy: 0.8826890756302521\n",
      "Epoch 3, Validation Accuracy: 0.8692436974789916\n",
      "Epoch 4, Validation Accuracy: 0.8709243697478991\n",
      "Epoch 5, Validation Accuracy: 0.8742857142857143\n",
      "Fold 3/5\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Epoch 1, Validation Accuracy: 0.853781512605042\n",
      "Epoch 2, Validation Accuracy: 0.8578151260504202\n",
      "Epoch 3, Validation Accuracy: 0.8554621848739495\n",
      "Epoch 4, Validation Accuracy: 0.8527731092436974\n",
      "Epoch 5, Validation Accuracy: 0.8547899159663865\n",
      "Fold 4/5\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Epoch 1, Validation Accuracy: 0.8581512605042017\n",
      "Epoch 2, Validation Accuracy: 0.8611764705882353\n",
      "Epoch 3, Validation Accuracy: 0.8594957983193278\n",
      "Epoch 4, Validation Accuracy: 0.8494117647058823\n",
      "Epoch 5, Validation Accuracy: 0.8534453781512605\n",
      "Fold 5/5\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Epoch 1, Validation Accuracy: 0.8460504201680672\n",
      "Epoch 2, Validation Accuracy: 0.8484033613445379\n",
      "Epoch 3, Validation Accuracy: 0.8484033613445379\n",
      "Epoch 4, Validation Accuracy: 0.8447058823529412\n",
      "Epoch 5, Validation Accuracy: 0.8463865546218488\n",
      "Model trained\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertForTokenClassification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Number of folds\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Convert 'semantic_roles' to a binary array\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_labels = mlb.fit_transform(train_df['semantic_roles'])\n",
    "\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained('l3cube-pune/hing-mbert', num_labels=num_labels)\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(train_tokens, binary_labels)):\n",
    "    print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    # Split the data into train and validation sets for this fold\n",
    "    train_fold_tokens = [train_tokens[i] for i in train_index]\n",
    "    train_fold_labels = [binary_labels[i] for i in train_index]\n",
    "\n",
    "    val_fold_tokens = [train_tokens[i] for i in val_index]\n",
    "    val_fold_labels = [binary_labels[i] for i in train_index]\n",
    "\n",
    "    # Create datasets and data loaders for this fold\n",
    "    train_fold_dataset = SRLDataset(train_fold_tokens, train_fold_labels)\n",
    "    val_fold_dataset = SRLDataset(val_fold_tokens, val_fold_labels)\n",
    "\n",
    "    train_fold_loader = DataLoader(train_fold_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_fold_loader = DataLoader(val_fold_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Train the model for NUM_EPOCHS\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_fold_loader):\n",
    "            inputs = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['semantic_roles']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=inputs.squeeze(1), attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate on the validation set after each epoch\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_fold_loader:\n",
    "                val_inputs = val_batch['input_ids']\n",
    "                val_attention_mask = val_batch['attention_mask']\n",
    "                val_labels = val_batch['semantic_roles']\n",
    "\n",
    "                val_outputs = model(input_ids=val_inputs.squeeze(1), attention_mask=val_attention_mask)\n",
    "                logits = val_outputs.logits\n",
    "\n",
    "                # Convert logits to predictions\n",
    "                predictions = torch.argmax(F.log_softmax(logits, dim=2), dim=2)\n",
    "\n",
    "                val_predictions.extend(predictions.tolist())\n",
    "                val_true_labels.extend(val_labels.tolist())\n",
    "\n",
    "        # Flatten the predictions and true labels for evaluation\n",
    "        flat_val_predictions = [label for sublist in val_predictions for label in sublist]\n",
    "        flat_val_true_labels = [label for sublist in val_true_labels for label in sublist]\n",
    "\n",
    "        val_accuracy = accuracy_score(flat_val_true_labels, flat_val_predictions)\n",
    "        print(f\"Epoch {epoch + 1}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(r\"C:\\Users\\bpsad\\Desktop\\TRAIN_DATA_PULLED\")\n",
    "print(\"Model trained\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6092cf35",
   "metadata": {},
   "source": [
    "# XLM RoBERTa Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "639719b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89734dc5b2c44a4a9d67b347e5c7838f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bpsad\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0d25f1b5c8494d8fa0eceed74d461c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef709e2277a542d0b8de842ff04e1c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdb77701d474c4db3010428a01fc217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "\n",
    "PAD_TOKEN_ID = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=25,  # Define your maximum sequence length\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "train_tokens = [tokenize_text(text) for text in train_df['tweet']]\n",
    "validation_tokens = [tokenize_text(text) for text in validation_df['tweet']]\n",
    "test_tokens = [tokenize_text(text) for text in test_df['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d231f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ac3895db1142bfb812956779d69611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Anaconda\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Epoch 1, Validation Accuracy: 0.8033613445378152\n",
      "Epoch 2, Validation Accuracy: 0.8346218487394959\n",
      "Epoch 3, Validation Accuracy: 0.852436974789916\n",
      "Epoch 4, Validation Accuracy: 0.8436974789915966\n",
      "Epoch 5, Validation Accuracy: 0.8342857142857143\n",
      "Fold 2/5\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Epoch 1, Validation Accuracy: 0.8544537815126051\n",
      "Epoch 2, Validation Accuracy: 0.8594957983193278\n",
      "Epoch 3, Validation Accuracy: 0.8484033613445379\n",
      "Epoch 4, Validation Accuracy: 0.8534453781512605\n",
      "Epoch 5, Validation Accuracy: 0.8490756302521009\n",
      "Fold 3/5\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Epoch 1, Validation Accuracy: 0.8389915966386554\n",
      "Epoch 2, Validation Accuracy: 0.8413445378151261\n",
      "Epoch 3, Validation Accuracy: 0.8403361344537815\n",
      "Epoch 4, Validation Accuracy: 0.8416806722689075\n",
      "Epoch 5, Validation Accuracy: 0.8447058823529412\n",
      "Fold 4/5\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Epoch 1, Validation Accuracy: 0.8436974789915966\n",
      "Epoch 2, Validation Accuracy: 0.8463865546218488\n",
      "Epoch 3, Validation Accuracy: 0.8440336134453782\n",
      "Epoch 4, Validation Accuracy: 0.8403361344537815\n",
      "Epoch 5, Validation Accuracy: 0.8440336134453782\n",
      "Fold 5/5\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Length of self.semantic_roles: 476\n",
      "Range of indices: range(0, 476)\n",
      "Epoch 1, Validation Accuracy: 0.8299159663865546\n",
      "Epoch 2, Validation Accuracy: 0.8305882352941176\n",
      "Epoch 3, Validation Accuracy: 0.8292436974789916\n",
      "Epoch 4, Validation Accuracy: 0.8295798319327731\n",
      "Epoch 5, Validation Accuracy: 0.8198319327731093\n",
      "Model trained\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertForTokenClassification\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Number of folds\n",
    "k_folds = 5\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Convert 'semantic_roles' to a binary array\n",
    "mlb = MultiLabelBinarizer()\n",
    "binary_labels = mlb.fit_transform(train_df['semantic_roles'])\n",
    "\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/xlm-roberta-base\", num_labels = num_labels)\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(train_tokens, binary_labels)):\n",
    "    print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    # Split the data into train and validation sets for this fold\n",
    "    train_fold_tokens = [train_tokens[i] for i in train_index]\n",
    "    train_fold_labels = [binary_labels[i] for i in train_index]\n",
    "\n",
    "    val_fold_tokens = [train_tokens[i] for i in val_index]\n",
    "    val_fold_labels = [binary_labels[i] for i in train_index]\n",
    "\n",
    "    # Create datasets and data loaders for this fold\n",
    "    train_fold_dataset = SRLDataset(train_fold_tokens, train_fold_labels)\n",
    "    val_fold_dataset = SRLDataset(val_fold_tokens, val_fold_labels)\n",
    "\n",
    "    train_fold_loader = DataLoader(train_fold_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_fold_loader = DataLoader(val_fold_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Train the model for NUM_EPOCHS\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_fold_loader):\n",
    "            inputs = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['semantic_roles']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=inputs.squeeze(1), attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate on the validation set after each epoch\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_fold_loader:\n",
    "                val_inputs = val_batch['input_ids']\n",
    "                val_attention_mask = val_batch['attention_mask']\n",
    "                val_labels = val_batch['semantic_roles']\n",
    "\n",
    "                val_outputs = model(input_ids=val_inputs.squeeze(1), attention_mask=val_attention_mask)\n",
    "                logits = val_outputs.logits\n",
    "\n",
    "                # Convert logits to predictions\n",
    "                predictions = torch.argmax(F.log_softmax(logits, dim=2), dim=2)\n",
    "\n",
    "                val_predictions.extend(predictions.tolist())\n",
    "                val_true_labels.extend(val_labels.tolist())\n",
    "\n",
    "        # Flatten the predictions and true labels for evaluation\n",
    "        flat_val_predictions = [label for sublist in val_predictions for label in sublist]\n",
    "        flat_val_true_labels = [label for sublist in val_true_labels for label in sublist]\n",
    "\n",
    "        val_accuracy = accuracy_score(flat_val_true_labels, flat_val_predictions)\n",
    "        print(f\"Epoch {epoch + 1}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(r\"C:\\Users\\bpsad\\Desktop\\TRAIN_DATA_PULLED\")\n",
    "print(\"Model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3859d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
